import streamlit as st
import pandas as pd

# Google Play Scraper ë° App Store Scraperë¥¼ ì‚¬ìš©í•œ ì•± ì •ë³´ ìŠ¤í¬ë˜í•‘
from google_play_scraper import search as gps_search, app as gps_app
from app_store_scraper import AppStore 

# ìŠ¤íŠ¸ë¦¼ë¦¿ íŒŒì¼ ì—°ê²° ìœ í‹¸ë¦¬í‹° (ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆì¼ ìˆ˜ ìˆìŒ)
from st_files_connection import FilesConnection
import os
#from google.cloud import storage  # Google Cloud Storage ì‘ì—…ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from io import StringIO
import asyncio

# Google OAuth ì¸ì¦ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from streamlit_oauth import OAuth2Component


#firebase ë¡œê·¸ì¸(login)
from firebase_admin import firestore


# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

from oauthlib.oauth2 import WebApplicationClient  # OAuth2 í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests


import numpy as np

import importlib.util
import sys

#GCP ì—°ê²°
#from google.cloud import storage



#ì‚¬ìš©ì ë¡œê·¸ì¸ DBì—°ê²°
import firebase_admin
from firebase_admin import credentials, firestore, storage


#ì¤€ì§€ë„
import torch
from transformers import GPT2Model, GPT2LMHeadModel, PreTrainedTokenizerFast
import faiss
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, SimpleRNN, GRU, Dense

#í´ëŸ¬ìŠ¤í„°
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from konlpy.tag import Okt
from sentence_transformers import SentenceTransformer, util
from datetime import datetime
from numpy import triu
from scipy.linalg import get_blas_funcs


#ê²Œì‹œíŒ DBì—°ê²° (ì»¤ë®¤ë‹ˆí‹°)
import sqlite3
import tempfile
from firebase_admin import storage #ì‚¬ìš©ìê°€ ì˜¬ë¦° íŒŒì¼ ì €ì¥í•˜ê¸° ìœ„í•œ Firebase storage ì—°ê²° ë¼ì´ë¸ŒëŸ¬ë¦¬
import firebase_admin
from firebase_admin import credentials, firestore, storage
from firebase_admin import credentials, storage  # storage ëª¨ë“ˆ ì¶”ê°€


#SNA
import networkx as nx
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.font_manager as fm
from collections import defaultdict

#LDAë¶€ë¶„
from collections import defaultdict
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from konlpy.tag import Okt
from gensim import corpora, models
from gensim.models import CoherenceModel
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
from PIL import Image
import numpy as np
import streamlit.components.v1 as components



#DGB Chatbot

import openai


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")

# ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ì´ˆê¸°í™” í•¨ìˆ˜
#  st.session_stateëŠ” Streamlit ì„œë²„ì˜ ë©”ëª¨ë¦¬ ë‚´ì—ì„œ ê´€ë¦¬
if 'posts' not in st.session_state:
    st.session_state['posts'] = []

 
 ########################################################3
 #ë©”ë‰´ ì‚¬ì´ë“œë°” ë¶€ë¶„
 
    
# ê° ë©”ë‰´ì— ëŒ€í•œ ë‚´ìš©ì€ í™”ë©´ì— í‘œì‹œë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì›í•˜ëŠ” ë©”ë‰´ë¥¼ ì„ íƒí•˜ì—¬ ë‚´ìš©ì„ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# dgb_image=st.sidebar.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F1f4539ac-bfe6-4b36-bf9c-6b6ad66e3300%2Fimage-removebg-preview_(14).png?table=block&id=ec2e0844-33f1-42d4-afdd-e42b67ab2ca9&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=710&userId=&cache=v2', width='400')   
# ì´ë¯¸ì§€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
image_path = 'https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F1f4539ac-bfe6-4b36-bf9c-6b6ad66e3300%2Fimage-removebg-preview_(14).png?table=block&id=ec2e0844-33f1-42d4-afdd-e42b67ab2ca9&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=710&userId=&cache=v2'

# ì‚¬ì´ë“œë°”ì— ì´ë¯¸ì§€ ì‚½ì…
st.sidebar.image(image_path, caption='DGB CASë¡œ ê³ ê°ë¶„ì„ì„ ì‰½ê³  ë¹ ë¥´ê²Œ!', use_column_width=True)
st.sidebar.markdown("---")
menu = st.sidebar.selectbox("MENU SELECTION", ["ğŸ í™ˆ", "ğŸ”ë¡œê·¸ì¸", "ğŸ“ì‚¬ìš©ë°©ë²•", "ğŸ“ë°ì´í„° ìˆ˜ì§‘&ì „ì²˜ë¦¬", "ğŸ“Ší´ëŸ¬ìŠ¤í„°ë§", "ğŸ“ˆì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„", "ğŸ“‰í† í”½ ëª¨ë¸ë§", "ğŸ’»ê¸°íšŒì ìˆ˜", "ğŸ™ğŸ»DGB CAS ì»¤ë®¤ë‹ˆí‹°","âŒ¨ï¸DGB Chatbot"])
st.sidebar.markdown("---")





# ê° ë©”ë‰´ì— ëŒ€í•œ ì„¤ëª… í…ìŠ¤íŠ¸
menu_text="""
## ğŸ’™DGB CAS MENUğŸ’™
"""

home_text = """
## ğŸ  í™ˆ

"""

login_text = """
## ğŸ” ë¡œê·¸ì¸

"""

usage_text = """
## ğŸ“ ì‚¬ìš©ë°©ë²•

"""

data_collection_text = """
## ğŸ“ ë°ì´í„° ìˆ˜ì§‘&ì „ì²˜ë¦¬

"""

clustering_text = """
## ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§

"""

social_network_analysis_text = """
## ğŸ“ˆ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„

"""

topic_modeling_text = """
## ğŸ“‰ í† í”½ ëª¨ë¸ë§

"""

opportunity_score_text = """
## ğŸ’» ê¸°íšŒì ìˆ˜

"""

bulletin_board_text = """
## ğŸ™ğŸ»â€â™€ï¸ DGB CAS ì»¤ë®¤ë‹ˆí‹°

"""

DGB_Chatbot_text = """
## âŒ¨ï¸ DGB Chatbot

"""

# ê° ë©”ë‰´ ì„¤ëª…ì„ ì‚¬ì´ë“œë°”ì— í‘œì‹œ
st.sidebar.markdown(menu_text, unsafe_allow_html=True)
st.sidebar.markdown(home_text, unsafe_allow_html=True)
st.sidebar.markdown(login_text, unsafe_allow_html=True)
st.sidebar.markdown(usage_text, unsafe_allow_html=True)
st.sidebar.markdown(data_collection_text, unsafe_allow_html=True)
st.sidebar.markdown(clustering_text, unsafe_allow_html=True)
st.sidebar.markdown(social_network_analysis_text, unsafe_allow_html=True)
st.sidebar.markdown(topic_modeling_text, unsafe_allow_html=True)
st.sidebar.markdown(opportunity_score_text, unsafe_allow_html=True)
st.sidebar.markdown(bulletin_board_text, unsafe_allow_html=True)
st.sidebar.markdown(DGB_Chatbot_text, unsafe_allow_html=True)


###############################################


# ì „ì—­ ë³€ìˆ˜ë¡œ ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
df = None

if menu == 'ğŸ í™ˆ':
    
    
    
    col1, col2 = st.columns([1, 3])  # ì²« ë²ˆì§¸ ì—´ì€ 1, ë‘ ë²ˆì§¸ ì—´ì€ 3ì˜ ë¹„ìœ¨ë¡œ ì„¤ì •
    
    html="""
        <div style='
        background-color: #f5fffa;
        color: white;
        padding: 20px;
        text-align: center;
        '>
            <img src='https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F1f4539ac-bfe6-4b36-bf9c-6b6ad66e3300%2Fimage-removebg-preview_(14).png?table=block&id=ec2e0844-33f1-42d4-afdd-e42b67ab2ca9&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=710&userId=&cache=v2' width='400'>
        </div>
    """

    st.markdown(html, unsafe_allow_html=True)
        
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F86784a8f-09d7-4c59-9e9a-b03473fb5333%2F75efd937-105b-4dc3-bd81-2b37465caf7c.png?table=block&id=13127fed-6039-47fa-8d35-9808d0c0678b&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=880&userId=&cache=v2',use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F245d7569-a3fa-4a48-89dd-4af7bc0dd39e%2FUntitled.png?table=block&id=206d5f21-03a1-4a35-b3e1-864266b898e4&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1280&userId=&cache=v2', use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fc0e6e1b2-feeb-4e9a-aed0-6bc182b1bcaf%2FUntitled.png?table=block&id=1b9f7cc1-035d-4b7d-b262-eaa687877a08&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1520&userId=&cache=v2', use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F7597632e-242d-4d9b-a6c2-66cdecc2353b%2FUntitled.png?table=block&id=3526b4a7-954d-4720-8860-9c8706511cb6&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1280&userId=&cache=v2', use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F5e17fc70-2d5c-4459-bd86-73469eec471a%2FUntitled.png?table=block&id=8fa66bd6-87d5-47f0-bfb8-bc2d2921e6ba&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1280&userId=&cache=v2',use_column_width=True)
 
 
elif menu == 'ğŸ”ë¡œê·¸ì¸':
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F72036241-44cc-4888-b714-f241a98fb706%2FUntitled.png?table=block&id=eb7729ef-4e30-400b-aafc-ad3e0ecba85b&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=870&userId=&cache=v2', use_column_width=True)

    # st.title('Google ë¡œê·¸ì¸')
    # st.markdown("<h1 ='color: blue;'>Google ë¡œê·¸ì¸</h1>", unsafe_allow_html=True)

    
    # Set environment variables
    AUTHORIZE_URL = "https://accounts.google.com/o/oauth2/auth"
    TOKEN_URL = "https://oauth2.googleapis.com/token"
    REFRESH_TOKEN_URL = "https://oauth2.googleapis.com/token"
    REVOKE_TOKEN_URL = "https://oauth2.googleapis.com/revoke"
    USERINFO_URL = "https://www.googleapis.com/oauth2/v3/userinfo"   # ì‚¬ìš©ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ì—…ë°ì´íŠ¸ëœ URL
    CLIENT_ID = "client_id"
    CLIENT_SECRET = "client_secret"
    REDIRECT_URI = "http://localhost:8501"
    SCOPE = "openid https://www.googleapis.com/auth/userinfo.email https://www.googleapis.com/auth/userinfo.profile"


    # Create OAuth2Component instance
    oauth2 = OAuth2Component(CLIENT_ID, CLIENT_SECRET, AUTHORIZE_URL, TOKEN_URL, REFRESH_TOKEN_URL, REVOKE_TOKEN_URL)


    # Firebase Admin SDK ì´ˆê¸°í™”
    if not firebase_admin._apps:
        cred = credentials.Certificate("C:\My\Competition\DGB\Final\streamlit\Google_oauth_Fire_key\dgb-user-login-database-firebase-adminsdk-lhbhr-146d46b243.json") # Firebase Admin SDK json íŒŒì¼ ê²½ë¡œ
        firebase_admin.initialize_app(cred, {
        'projectId': 'dgb-user-login-database',
        })
        firebase_admin.initialize_app(cred)

    # Firestore DB ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    db = firebase_admin.firestore.client()

    # ì‚¬ìš©ì ì •ë³´ Firestoreì— ì €ì¥ ë˜ëŠ” ì—…ë°ì´íŠ¸
    def save_user_info(user_info):
        # Firestoreì˜ users ì»¬ë ‰ì…˜ì— ì‚¬ìš©ì ì •ë³´ ì €ì¥
        user_ref = db.collection(u'users').document(user_info['email'])
        user_ref.set({
            u'email': user_info['email'],
            u'name': user_info['name'],
            u'picture': user_info['picture']
        })

    # Google ë¡œê·¸ì¸ ì„±ê³µ í›„ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸° ë° ì €ì¥
    if 'token' in st.session_state:
        # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        access_token = st.session_state['token']['access_token']
        headers = {'Authorization': f'Bearer {access_token}'}
        response = requests.get(USERINFO_URL, headers=headers)
        user_info = response.json()
        
        # Firestoreì— ì‚¬ìš©ì ì •ë³´ ì €ì¥ ë˜ëŠ” ì—…ë°ì´íŠ¸
        save_user_info(user_info)
        
 
    
    # ì„¸ì…˜ ìƒíƒœì—ì„œ í† í°ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'token' not in st.session_state:
        left, center, right = st.columns([1,2,1])
        with center:
            with st.container():
                
                    st.markdown("#####")
                    col1, col2, col3 = st.columns([1,2,1])
                    with col2:  # ì¤‘ì•™ ì»¬ëŸ¼ì—ì„œ ìš”ì†Œë“¤ì„ ì¶”ê°€
                        
                    # ì—†ìœ¼ë©´ Google ì•„ì´ì½˜ê³¼ í•¨ê»˜ ì¸ì¦ ë²„íŠ¼ í‘œì‹œ
                        st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fd4dbd66d-c2db-43bc-89f0-6c1a06b028f2%2FUntitled.png?table=block&id=813ed66c-d30c-47f9-a970-09825eefb4e9&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=850&userId=&cache=v2', width=400) # ì´ë¯¸ì§€ URLê³¼ ë„ˆë¹„ë¥¼ ì ì ˆíˆ ì¡°ì •í•´ì£¼ì„¸ìš”.
                        
                        result = oauth2.authorize_button("Continue with Google", REDIRECT_URI, SCOPE, icon="data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' viewBox='0 0 48 48'%3E%3Cdefs%3E%3Cpath id='a' d='M44.5 20H24v8.5h11.8C34.7 33.9 30.1 37 24 37c-7.2 0-13-5.8-13-13s5.8-13 13-13c3.1 0 5.9 1.1 8.1 2.9l6.4-6.4C34.6 4.1 29.6 2 24 2 11.8 2 2 11.8 2 24s9.8 22 22 22c11 0 21-8 21-22 0-1.3-.2-2.7-.5-4z'/%3E%3C/defs%3E%3CclipPath id='b'%3E%3Cuse xlink:href='%23a' overflow='visible'/%3E%3C/clipPath%3E%3Cpath clip-path='url(%23b)' fill='%23FBBC05' d='M0 37V11l17 13z'/%3E%3Cpath clip-path='url(%23b)' fill='%23EA4335' d='M0 11l17 13 7-6.1L48 14V0H0z'/%3E%3Cpath clip-path='url(%23b)' fill='%2334A853' d='M0 37l30-23 7.9 1L48 0v48H0z'/%3E%3Cpath clip-path='url(%23b)' fill='%234285F4' d='M48 48L17 24l-4-3 35-10z'/%3E%3C/svg%3E", use_container_width=True)
                        
                        if result and 'token' in result:
                            # ì¸ì¦ì´ ì„±ê³µí•˜ë©´ ì„¸ì…˜ ìƒíƒœì— í† í° ì €ì¥
                            st.session_state.token = result.get('token')
                            st.rerun()
    #
    else:
        left, center, right = st.columns([1,2,1])
        with center:
            with st.container():
                # ë¡œê·¸ì•„ì›ƒ í¼ ì‹œì‘
                with st.form("logout_form"):  # í¼ ì‹œì‘
                    col1, col2, col3 = st.columns([1,2,1])
                    with col2:  # ì¤‘ì•™ ì»¬ëŸ¼ì—ì„œ ìš”ì†Œë“¤ì„ ì¶”ê°€
                        # ì¤‘ì•™ ì´ë¯¸ì§€ ì¶”ê°€
                        st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F8c634fb7-8ee8-492b-a181-3345b5b7375a%2FUntitled.png?table=block&id=28c51d75-42ab-4ccc-8d49-b4d015751e77&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=850&userId=&cache=v2', width=400)
                        # ì‚¬ìš©ì í”„ë¡œí•„ ì‚¬ì§„ê³¼ í™˜ì˜ ë©”ì‹œì§€ë¥¼ Markdownê³¼ HTMLì„ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ì¤„ì— í‘œì‹œ
                        st.markdown(f"""
                        <div style="display:flex;align-items:center;">
                            <img src="{user_info['picture']}" width="80" style="margin-right: 10px;"/>
                            <span>í™˜ì˜í•©ë‹ˆë‹¤, {user_info['name']}ë‹˜</span>
                        </div>
                        """, unsafe_allow_html=True)

                    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
                    logout_button = st.form_submit_button("ë¡œê·¸ì•„ì›ƒ")
                if logout_button:
                    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´, í† í°ì„ íê¸°í•˜ê³  ì„¸ì…˜ ìƒíƒœì—ì„œ í† í° ì‚­ì œ
                    oauth2.revoke_token(st.session_state['token'])
                    del st.session_state['token']
                    st.success("ë¡œê·¸ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.experimental_rerun()
                    
        st.markdown("---")
        # my pageë¶€ë¶„
        st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F17d488a8-cfb4-4c13-9efe-1148e273185d%2FUntitled.png?table=block&id=a3da8546-f591-4d9f-b8f0-cc50a1b8f3fc&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=870&userId=&cache=v2',use_column_width=True)
        # íŒŒì¼ ì—…ë¡œë“œ ì¸í„°í˜ì´ìŠ¤ ì¶”ê°€
        uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=None)
        if uploaded_file is not None:
            file_info = {
                'name': uploaded_file.name,
                'type': uploaded_file.type,
                'size': uploaded_file.size,
            }
            file_ref = db.collection(u'files').document(uploaded_file.name)
            file_ref.set(file_info)
            st.success(f"'{uploaded_file.name}' íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # Firestoreì—ì„œ 'files' ì»¬ë ‰ì…˜ì˜ ëª¨ë“  íŒŒì¼ ì •ë³´ ì¡°íšŒ
        docs = db.collection(u'files').stream()
        file_list = []
        for doc in docs:
            doc_dict = doc.to_dict()
            doc_dict['id'] = doc.id  # ë¬¸ì„œ ID ì¶”ê°€
            file_list.append(doc_dict)

        if file_list:
            df_files = pd.DataFrame(file_list)
            st.write("ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡:")
            st.dataframe(df_files)  # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ë¥¼ í‘œë¡œ í‘œì‹œ

            # ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì„ íƒí•˜ì—¬ ì‚­ì œí•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë¶€ë¶„
            selected_file = st.selectbox("ì‚­ì œí•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", df_files['name'])
            if st.button("íŒŒì¼ ì‚­ì œ"):
                db.collection(u'files').document(selected_file).delete()
                st.success(f"'{selected_file}' íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # íŒŒì¼ ì—´ëŒ ê¸°ëŠ¥ (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ íŒŒì¼ì„ ì„ì‹œ ì €ì¥ì†Œì— ì €ì¥í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©)
            open_file = st.selectbox("ì—´ëŒí•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", df_files['name'])
            if st.button("íŒŒì¼ ì—´ëŒ"):
                doc = db.collection(u'files').document(open_file).get()
                if doc.exists:
                    file_info = doc.to_dict()
                    file_path = os.path.join(tempfile.gettempdir(), file_info['name'])
                    # ì—¬ê¸°ì„œëŠ” íŒŒì¼ì´ ì„œë²„ì— ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , í•´ë‹¹ ê²½ë¡œì—ì„œ íŒŒì¼ì„ ì°¾ì•„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•©ë‹ˆë‹¤.
                    # ì‹¤ì œë¡œëŠ” íŒŒì¼ì„ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
                    st.write(f"íŒŒì¼ ê²½ë¡œ: {file_path}")
                    # íŒŒì¼ì„ ì—´ëŒí•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë§í¬ ì œê³µ ë“±ì˜ ì¶”ê°€ ì‘ì—…ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


                                
                    
                    
elif menu == 'ğŸ“ì‚¬ìš©ë°©ë²•':
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F339fa6c2-c453-4c3f-a063-9ce8dc533c64%2FUntitled.png?table=block&id=088cf27e-a04c-41c2-a884-d4f4442f6f06&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', use_column_width=True)
    
    # st.title('ì‚¬ìš©ë°©ë²•')
    st.title('DGB ê³ ê° ë¶„ì„ ì‹œìŠ¤í…œ ì‚¬ìš©ë°©ë²• ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ íë¦„ìœ¼ë¡œ ë¶„ì„ì´ ì§„í–‰ë©ë‹ˆë‹¤.')
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Ffedded3b-f946-4a77-b700-fd5bcf480d75%2FUntitled.png?table=block&id=37126797-6dff-4319-9e04-4f511ba68b9c&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1280&userId=&cache=v2',use_column_width=True)
    
   
  
    #ë°ì´í„° ìˆ˜ì§‘&ì „ì²˜ë¦¬&ì¤€ì§€ë„
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F689c5e7d-5275-46dc-bd17-ad2ffffe66cf%2FUntitled.png?table=block&id=b984c3b9-6461-40c5-b497-35f161b400dc&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1360&userId=&cache=v2', use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F3ceea4b0-b1f7-47e7-89c2-1b6cc5517251%2FUntitled.png?table=block&id=585de0d0-1332-4b19-8ba1-154ac8afc77e&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=850&userId=&cache=v2', use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F0a3854ca-2279-4352-94d9-2e8e004b627a%2FUntitled.png?table=block&id=d905cb50-b5e7-45fa-8a40-4ba06b427c85&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=850&userId=&cache=v2', use_column_width=True)
    
    
    
    #í´ëŸ¬ìŠ¤í„°ë§
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F1456364b-eefa-4158-b9d1-106f8356e69e%2FUntitled.png?table=block&id=7e20f50a-85d5-47af-b5cd-3971e0a2db96&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=850&userId=&cache=v2', use_column_width=True)
    #ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fea0902e2-df38-46f9-8b3c-80fdd01e2abc%2FUntitled.png?table=block&id=6e413406-97a7-4ee2-b9fc-6af336683b96&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1360&userId=&cache=v2', use_column_width=True)
    #í† í”½ ëª¨ë¸ë§
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F114f8151-802b-4f9b-a95e-98072a74a92a%2FUntitled.png?table=block&id=c8cf6409-bde9-458a-999a-6c09213d3fc0&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1290&userId=&cache=v2', use_column_width=True)
    #ê¸°íšŒì ìˆ˜ ë„ì¶œ
    #st.image(' ', use_column_width=True)
    
elif menu == 'ğŸ“ë°ì´í„° ìˆ˜ì§‘&ì „ì²˜ë¦¬':
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fd07544e2-b5d7-4900-8336-667702df8494%2FUntitled.png?table=block&id=c6a18559-169b-4873-854a-b4e6a68af24f&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=870&userId=&cache=v2', use_column_width=True)
   
    # st.title('ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬')

    # ì‚¬ìš©ì ì„ íƒ ì˜µì…˜
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F1601552c-387c-43c9-9ce5-ac3fb961c99d%2FUntitled.png?table=block&id=4af6f754-9488-4c00-ac24-753ed86c8f76&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', width=500)
   
    
    st.subheader("3ê°€ì§€ì˜ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ì„ íƒ í›„ 'Add filters' ê¸°ëŠ¥ìœ¼ë¡œ ì›í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


    def filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:
        """
        Adds a UI on top of a dataframe to let viewers filter columns

        Args:
            df (pd.DataFrame): Original dataframe

        Returns:
            pd.DataFrame: Filtered dataframe
        """
        modify = st.checkbox("Add filters")

        if not modify:
            return df

        df = df.copy()

        # Filter columns based on user input
        modification_container = st.container()

        with modification_container:
            to_filter_columns = st.multiselect("Filter dataframe on", df.columns)
            for column in to_filter_columns:
                left, right = st.columns((1, 20))
                left.write("â†³")
                # Check column data type and apply appropriate filtering
                user_input = right.text_input(f"Filter by {column}")
                if user_input:
                    df = df[df[column].str.contains(user_input)]

        return df
    
    
    
    
    
    
    
    
    
    
    # st.markdown("<h1 style='color: blue;'>ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ì„ íƒ</h1>", unsafe_allow_html=True)

    option = st.radio('ì›í•˜ëŠ” ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”.',
                    ('1) ì§ì ‘ íŒŒì¼ ì—…ë¡œë“œ', '2) ì•± ìŠ¤í† ì–´ ê³ ê° ë¦¬ë·° í¬ë¡¤ë§', '3) GCPì—°ê²°-->ë¯¸ë¦¬ ìˆ˜ì§‘ëœ ë°ì´í„°ë² ì´ìŠ¤'))

    # st.markdown("---")  # ì„¹ì…˜ êµ¬ë¶„ì„ ìœ„í•œ ìˆ˜í‰ì„ 
    
    

    if option == '1) ì§ì ‘ íŒŒì¼ ì—…ë¡œë“œ':
        st.subheader('1) ì§ì ‘ íŒŒì¼ ì—…ë¡œë“œ')
        uploaded_file = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=['csv', 'xlsx'], key="unique_key_1")
        if uploaded_file is not None:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            elif uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)
            st.write("ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë°ì´í„°í”„ë ˆì„:")
            st.dataframe(df)
            # Filtered dataframe
            st.dataframe(filter_dataframe(df))
            
            
            

    elif option == '2) ì•± ìŠ¤í† ì–´ ê³ ê° ë¦¬ë·° í¬ë¡¤ë§':
        st.subheader('2) ì•± ìŠ¤í† ì–´ ê³ ê° ë¦¬ë·° í¬ë¡¤ë§')
        # ì—¬ê¸°ì— ì•± ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ë§ ê´€ë ¨ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
        # ì˜ˆì œ ì½”ë“œì—ì„œ ì œê³µëœ ì•± ê²€ìƒ‰ ë° ë¦¬ë·° ê²€ìƒ‰ ì½”ë“œë¥¼ ì—¬ê¸°ì— ì‚½ì…í•˜ì„¸ìš”.
        st.text('ì•± IDë¥¼ ê²€ìƒ‰ í›„ ì•± ë¦¬ë·°ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')
            
        # ìŠ¤íŠ¸ë¦¼ë¦¿ íƒ€ì´í‹€ ì„¤ì •
        # st.text('ì•± ID ê²€ìƒ‰')

        # ì•± ì´ë¦„ ì…ë ¥ ë°›ê¸°
        app_name = st.text_input('ì•± ID ê²€ìƒ‰: ì•± ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”', '')

        # ì•± ê²€ìƒ‰ ë° ê²°ê³¼ í‘œì‹œ
        if app_name:
                    st.subheader('ê²€ìƒ‰ ê²°ê³¼')
                    results = gps_search(app_name, lang='ko', country='kr')
                    for result in results[:5]:  # ìµœëŒ€ 5ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
                        st.write(f"ì•± ì´ë¦„: {result['title']}, ì•± ID: {result['appId']}")

                    # ë‘ë²ˆì§¸  í¼
                    st.subheader('ì•± ë¦¬ë·° ê²€ìƒ‰')

                    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
                    app_input = st.text_input('ì•± ì´ë¦„ ë˜ëŠ” ì•± IDë¥¼ ì…ë ¥í•˜ì„¸ìš”:', '')

                    # ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
                    df_reviews = pd.DataFrame()

                    # ê²€ìƒ‰ëœ ì•± ë¦¬ë·° í‘œì‹œ
                    if app_input:
                        try:
                            # ì•± ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
                            app = AppStore(country="kr", app_name=app_input)
                            app.review(how_many=3)  # ìµœê·¼ 3ê°œì˜ ë¦¬ë·°ë§Œ ê°€ì ¸ì˜´
                            
                            # ë¦¬ë·°ê°€ ìˆëŠ” ê²½ìš° ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
                            if app.reviews:
                                df_reviews = pd.DataFrame(app.reviews)
                                st.dataframe(df_reviews)
                                st.dataframe(filter_dataframe(df_reviews))
                        except Exception as e:
                            st.error(f'ì•± ë¦¬ë·°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}')
    
    
    
 
    elif option == '3) GCPì—°ê²°-->ë¯¸ë¦¬ ìˆ˜ì§‘ëœ ë°ì´í„°ë² ì´ìŠ¤':
        from google.cloud import storage
        st.subheader('3) GCPì—°ê²°-->ë¯¸ë¦¬ ìˆ˜ì§‘ëœ ë°ì´í„°ë² ì´ìŠ¤')
        
        def connect_to_gcs():
            # JSON í‚¤ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ GCS í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±
            client_gcs = storage.Client.from_service_account_json(r'C:\My\Competition\DGB\Final\streamlit\dgb-api-test-fb40a3e53580.json')
            return client_gcs

        def load_dataframe_from_gcs(client_gcs, bucket_name, file_name):
            # GCS ë²„í‚·ì—ì„œ íŒŒì¼ ì½ì–´ì˜¤ê¸°
            bucket = client_gcs.get_bucket(bucket_name)
            blob = bucket.blob(file_name)
            data = blob.download_as_string().decode("utf-8")  # íŒŒì¼ì„ ë°”ì´íŠ¸ë¡œ ì½ì–´ì™€ ë¬¸ìì—´ë¡œ ë””ì½”ë”©
            data_io = StringIO(data)
            return pd.read_csv(data_io)

        def main():
            st.title('')
            # GCS ì—°ê²°
            client_gcs = connect_to_gcs()

            if client_gcs:  # client_gcsê°€ Noneì´ ì•„ë‹ˆë©´ ì‹¤í–‰
                # GCS ë²„í‚· ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                buckets = client_gcs.list_buckets()
                bucket_options = [bucket.name for bucket in buckets]

                # ì‚¬ìš©ìê°€ ì„ íƒí•œ ë²„í‚· ì´ë¦„
                selected_bucket_name = st.selectbox("ë²„í‚· ì„ íƒ", bucket_options)

                if selected_bucket_name:
                    # ì„ íƒí•œ ë²„í‚·ì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                    bucket = client_gcs.get_bucket(selected_bucket_name)
                    blobs = bucket.list_blobs()
                    file_options = [blob.name for blob in blobs]

                    # ì‚¬ìš©ìê°€ ì„ íƒí•œ íŒŒì¼ëª…
                    selected_file_name = st.selectbox("íŒŒì¼ ì„ íƒ", file_options)

                    if selected_file_name and st.button('ìˆ˜ì§‘'):
                        # ì„ íƒí•œ íŒŒì¼ì—ì„œ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë¡œë“œ
                        df = load_dataframe_from_gcs(client_gcs, selected_bucket_name, selected_file_name)
                        # ë°ì´í„°í”„ë ˆì„ ì¶œë ¥
                        st.write(df)
                        st.dataframe(filter_dataframe(df))

        if __name__ == '__main__':
            main()

            
            
    st.markdown("---")  # ì„¸ë¡œ êµ¬ë¶„ì„ 
    
    #ì „ì²˜ë¦¬ ë¶€ë¶„ ----------------------------------------------------------
    # st.title('ì „ì²˜ë¦¬')
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F890fc8e1-45bb-4718-a9bb-b7e2ec8e36de%2FUntitled.png?table=block&id=64a95fe1-944a-4d83-b10b-2fd436d9c263&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', width=500)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F616ab9b7-d048-4616-9f74-384ead265592%2FUntitled.png?table=block&id=aef18537-2845-4012-b892-116bbdfa71a4&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', use_column_width=True)
    
  
    from konlpy.tag import Okt
    import re
    
    st.subheader("ì „ì²˜ë¦¬ë¥¼ í•˜ê¸° ìœ„í•´ì„  keyword, date, text, tok_textê°€ í•„ìš”í•©ë‹ˆë‹¤. ì»¬ëŸ¼ ìˆ˜ì •/ìƒì„±/ì‚­ì œ ê¸°ëŠ¥ì„ ì´ìš©í•´ì£¼ì„¸ìš”.")


    # íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
    uploaded_file = st.file_uploader("íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", type=["csv"])

    if uploaded_file is not None:
        # ì—…ë¡œë“œëœ íŒŒì¼ì„ Pandas DataFrameìœ¼ë¡œ ë¡œë“œ
        df = pd.read_csv(uploaded_file)
        
        # ë°ì´í„° í”„ë ˆì„ì„ í™”ë©´ì— í‘œì‹œ
        st.write(df)
        
        # Radio ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” ê¸°ëŠ¥ ì„ íƒ
        option = st.radio('ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ì„ íƒí•˜ì„¸ìš”.',
                        ('ì»¬ëŸ¼ëª… ìˆ˜ì •', 'ì»¬ëŸ¼ ìƒì„±', 'ì»¬ëŸ¼ ì‚­ì œ'))
        
        # ì»¬ëŸ¼ëª… ìˆ˜ì • ê¸°ëŠ¥
        if option == 'ì»¬ëŸ¼ëª… ìˆ˜ì •':
            old_column_name = st.selectbox("ìˆ˜ì •í•  ì»¬ëŸ¼ëª…ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", df.columns)
            new_column_name = st.text_input("ìƒˆ ì»¬ëŸ¼ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "")
            if st.button("ì»¬ëŸ¼ëª… ìˆ˜ì •"):
                df.rename(columns={old_column_name: new_column_name}, inplace=True)
                st.success(f"'{old_column_name}' ì»¬ëŸ¼ëª…ì´ '{new_column_name}'(ìœ¼)ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.write(df)
        
        # ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€ ê¸°ëŠ¥
        elif option == 'ì»¬ëŸ¼ ìƒì„±':
            new_col_name = st.text_input("ìƒˆë¡œ ìƒì„±í•  ì»¬ëŸ¼ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", key='new_col')
            new_col_value = st.text_input("ìƒˆ ì»¬ëŸ¼ì˜ ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", key='new_col_val')
            if st.button("ì»¬ëŸ¼ ìƒì„±", key='create_col'):
                df[new_col_name] = new_col_value
                st.success(f"'{new_col_name}' ì»¬ëŸ¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.write(df)
        
        # ì»¬ëŸ¼ ì‚­ì œ ê¸°ëŠ¥
        elif option == 'ì»¬ëŸ¼ ì‚­ì œ':
            del_column_name = st.selectbox("ì‚­ì œí•  ì»¬ëŸ¼ëª…ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", df.columns, key='del_col')
            if st.button("ì»¬ëŸ¼ ì‚­ì œ", key='delete_col'):
                df.drop(del_column_name, axis=1, inplace=True)
                st.success(f"'{del_column_name}' ì»¬ëŸ¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.write(df)


        # ì „ì²˜ë¦¬ ì‹œì‘ ë²„íŠ¼
        import numpy as np
        import re
        from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma
        from datetime import datetime

        
        if st.button("ì „ì²˜ë¦¬ ì‹œì‘"):
            

            # 'text' ì»¬ëŸ¼ì„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í† í°í™”í•˜ì—¬ 'tok_text' ì»¬ëŸ¼ ìƒì„±
            df['tok_text'] = df['text'].fillna('').apply(lambda x: " ".join(x.split()))
            
            # ì¤‘ë³µê°’ ì œê±°
            df = df.drop_duplicates(subset=['text'])

            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            df['text'] = df['text'].fillna('')
            
            
            
            
            # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜
            stopwords = ["ê°€", "ê°€ê¹ŒìŠ¤ë¡œ", "ê°€ë ¹", "ê°", "ê°ê°", "ê°ì", "ê°ì¢…", "ê°–ê³ ë§í•˜ìë©´", "ê°™ë‹¤", "ê°™ì´", "ê°œì˜ì¹˜ì•Šê³ ", "ê±°ë‹ˆì™€", "ê±°ë°”", "ê±°ì˜", "ê²ƒ", "ê²ƒê³¼ ê°™ì´", "ê²ƒë“¤", "ê²Œë‹¤ê°€", "ê²Œìš°ë‹¤", "ê²¨ìš°", "ê²¬ì§€ì—ì„œ", "ê²°ê³¼ì— ì´ë¥´ë‹¤", "ê²°êµ­", "ê²°ë¡ ì„ ë‚¼ ìˆ˜ ìˆë‹¤", "ê²¸ì‚¬ê²¸ì‚¬", "ê³ ë ¤í•˜ë©´", "ê³ ë¡œ", "ê³§", "ê³µë™ìœ¼ë¡œ", "ê³¼", "ê³¼ì—°", "ê´€ê³„ê°€ ìˆë‹¤", "ê´€ê³„ì—†ì´", "ê´€ë ¨ì´ ìˆë‹¤", "ê´€í•˜ì—¬", "ê´€í•œ", "ê´€í•´ì„œëŠ”", "êµ¬", "êµ¬ì²´ì ìœ¼ë¡œ", "êµ¬í† í•˜ë‹¤", "ê·¸", "ê·¸ë“¤", "ê·¸ë•Œ", "ê·¸ë˜", "ê·¸ë˜ë„", "ê·¸ë˜ì„œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ëŸ¬ë‹ˆ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ê·¸ëŸ¬ë©´", "ê·¸ëŸ¬ë¯€ë¡œ", "ê·¸ëŸ¬í•œì¦‰", "ê·¸ëŸ° ê¹Œë‹­ì—", "ê·¸ëŸ°ë°", "ê·¸ëŸ°ì¦‰", "ê·¸ëŸ¼", "ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ", "ê·¸ë ‡ê²Œ í•¨ìœ¼ë¡œì¨", "ê·¸ë ‡ì§€", "ê·¸ë ‡ì§€ ì•Šë‹¤ë©´", "ê·¸ë ‡ì§€ ì•Šìœ¼ë©´", "ê·¸ë ‡ì§€ë§Œ", "ê·¸ë ‡ì§€ì•Šìœ¼ë©´", "ê·¸ë¦¬ê³ ", "ê·¸ë¦¬í•˜ì—¬", "ê·¸ë§Œì´ë‹¤", "ê·¸ì— ë”°ë¥´ëŠ”", "ê·¸ìœ„ì—", "ê·¸ì €", "ê·¸ì¤‘ì—ì„œ", "ê·¸ì¹˜ì§€ ì•Šë‹¤", "ê·¼ê±°ë¡œ", "ê·¼ê±°í•˜ì—¬", "ê¸°ëŒ€ì—¬", "ê¸°ì ìœ¼ë¡œ", "ê¸°ì¤€ìœ¼ë¡œ", "ê¸°íƒ€", "ê¹Œë‹­ìœ¼ë¡œ", "ê¹Œì•…", "ê¹Œì§€", "ê¹Œì§€ ë¯¸ì¹˜ë‹¤", "ê¹Œì§€ë„", "ê½ˆë‹¹", "ë™ë™", "ë¼ìµ", "ë‚˜", "ë‚˜ë¨¸ì§€ëŠ”", "ë‚¨ë“¤", "ë‚¨ì§“", "ë„ˆ", "ë„ˆí¬", "ë„ˆí¬ë“¤", "ë„¤", "ë„·", "ë…„", "ë…¼í•˜ì§€ ì•Šë‹¤", "ë†€ë¼ë‹¤", "ëˆ„ê°€ ì•Œê² ëŠ”ê°€", "ëˆ„êµ¬", "ë‹¤ë¥¸", "ë‹¤ë¥¸ ë°©ë©´ìœ¼ë¡œ", "ë‹¤ë§Œ", "ë‹¤ì„¯", "ë‹¤ì†Œ", "ë‹¤ìˆ˜", "ë‹¤ì‹œ ë§í•˜ìë©´", "ë‹¤ì‹œë§í•˜ë©´", "ë‹¤ìŒ", "ë‹¤ìŒì—", "ë‹¤ìŒìœ¼ë¡œ", "ë‹¨ì§€", "ë‹µë‹¤", "ë‹¹ì‹ ", "ë‹¹ì¥", "ëŒ€ë¡œ í•˜ë‹¤", "ëŒ€í•˜ë©´", "ëŒ€í•˜ì—¬", "ëŒ€í•´ ë§í•˜ìë©´", "ëŒ€í•´ì„œ", "ëŒ•ê·¸", "ë”êµ¬ë‚˜", "ë”êµ°ë‹¤ë‚˜", "ë”ë¼ë„", "ë”ë¶ˆì–´", "ë”ìš±ë”", "ë”ìš±ì´ëŠ”", "ë„ë‹¬í•˜ë‹¤", "ë„ì°©í•˜ë‹¤", "ë™ì‹œì—", "ë™ì•ˆ", "ëœë°”ì—ì•¼", "ëœì´ìƒ", "ë‘ë²ˆì§¸ë¡œ", "ë‘˜", "ë‘¥ë‘¥", "ë’¤ë”°ë¼", "ë’¤ì´ì–´", "ë“ ê°„ì—", "ë“¤", "ë“±", "ë“±ë“±", "ë”©ë™", "ë”°ë¼", "ë”°ë¼ì„œ", "ë”°ìœ„", "ë”°ì§€ì§€ ì•Šë‹¤", "ë”±", "ë•Œ", "ë•Œê°€ ë˜ì–´", "ë•Œë¬¸ì—", "ë˜", "ë˜í•œ", "ëšëš", "ë¼ í•´ë„", "ë ¹", "ë¡œ", "ë¡œ ì¸í•˜ì—¬", "ë¡œë¶€í„°", "ë¡œì¨", "ë¥™", "ë¥¼", "ë§ˆìŒëŒ€ë¡œ", "ë§ˆì €", "ë§ˆì €ë„", "ë§ˆì¹˜", "ë§‰ë¡ í•˜ê³ ", "ë§Œ ëª»í•˜ë‹¤", "ë§Œì•½", "ë§Œì•½ì—", "ë§Œì€ ì•„ë‹ˆë‹¤", "ë§Œì´ ì•„ë‹ˆë‹¤", "ë§Œì¼", "ë§Œí¼", "ë§í•˜ìë©´", "ë§í• ê²ƒë„ ì—†ê³ ", "ë§¤", "ë§¤ë²ˆ", "ë©”ì“°ê²ë‹¤", "ëª‡", "ëª¨", "ëª¨ë‘", "ë¬´ë µ", "ë¬´ë¦ì“°ê³ ", "ë¬´ìŠ¨", "ë¬´ì—‡", "ë¬´ì—‡ë•Œë¬¸ì—", "ë¬¼ë¡ ", "ë°", "ë°”ê¾¸ì–´ë§í•˜ë©´", "ë°”ê¾¸ì–´ë§í•˜ìë©´", "ë°”ê¾¸ì–´ì„œ ë§í•˜ë©´", "ë°”ê¾¸ì–´ì„œ í•œë‹¤ë©´", "ë°”ê¿” ë§í•˜ë©´", "ë°”ë¡œ", "ë°”ì™€ê°™ì´", "ë°–ì— ì•ˆëœë‹¤", "ë°˜ëŒ€ë¡œ", "ë°˜ëŒ€ë¡œ ë§í•˜ìë©´", "ë°˜ë“œì‹œ", "ë²„ê¸ˆ", "ë³´ëŠ”ë°ì„œ", "ë³´ë‹¤ë”", "ë³´ë“œë“", "ë³¸ëŒ€ë¡œ", "ë´", "ë´ë¼", "ë¶€ë¥˜ì˜ ì‚¬ëŒë“¤", "ë¶€í„°", "ë¶ˆêµ¬í•˜ê³ ", "ë¶ˆë¬¸í•˜ê³ ", "ë¶•ë¶•", "ë¹„ê±±ê±°ë¦¬ë‹¤", "ë¹„êµì ", "ë¹„ê¸¸ìˆ˜ ì—†ë‹¤", "ë¹„ë¡œì†Œ", "ë¹„ë¡", "ë¹„ìŠ·í•˜ë‹¤", "ë¹„ì¶”ì–´ ë³´ì•„", "ë¹„í•˜ë©´", "ë¿ë§Œ ì•„ë‹ˆë¼", "ë¿ë§Œì•„ë‹ˆë¼", "ë¿ì´ë‹¤", "ì‚ê±±", "ì‚ê±±ê±°ë¦¬ë‹¤", "ì‚¬", "ì‚¼", "ìƒëŒ€ì ìœ¼ë¡œ ë§í•˜ìë©´", "ìƒê°í•œëŒ€ë¡œ", "ì„¤ë ¹", "ì„¤ë§ˆ", "ì„¤ì‚¬", "ì…‹", "ì†Œìƒ", "ì†Œì¸", "ì†¨", "ì‰¿", "ìŠµë‹ˆê¹Œ", "ìŠµë‹ˆë‹¤", "ì‹œê°", "ì‹œê°„", "ì‹œì‘í•˜ì—¬", "ì‹œì´ˆì—", "ì‹œí‚¤ë‹¤", "ì‹¤ë¡œ", "ì‹¬ì§€ì–´", "ì•„", "ì•„ë‹ˆ", "ì•„ë‹ˆë‚˜ë‹¤ë¥¼ê°€", "ì•„ë‹ˆë¼ë©´", "ì•„ë‹ˆë©´", "ì•„ë‹ˆì—ˆë‹¤ë©´", "ì•„ë˜ìœ—", "ì•„ë¬´ê±°ë‚˜", "ì•„ë¬´ë„", "ì•„ì•¼", "ì•„ìš¸ëŸ¬", "ì•„ì´", "ì•„ì´ê³ ", "ì•„ì´êµ¬", "ì•„ì´ì•¼", "ì•„ì´ì¿ ", "ì•„í•˜", "ì•„í™‰", "ì•ˆ ê·¸ëŸ¬ë©´", "ì•Šê¸° ìœ„í•˜ì—¬", "ì•Šê¸° ìœ„í•´ì„œ", "ì•Œ ìˆ˜ ìˆë‹¤", "ì•Œì•˜ì–´", "ì•—", "ì•ì—ì„œ", "ì•ì˜ê²ƒ", "ì•¼", "ì•½ê°„", "ì–‘ì", "ì–´", "ì–´ê¸°ì—¬ì°¨", "ì–´ëŠ", "ì–´ëŠ ë…„ë„", "ì–´ëŠê²ƒ", "ì–´ëŠê³³", "ì–´ëŠë•Œ", "ì–´ëŠìª½", "ì–´ëŠí•´", "ì–´ë””", "ì–´ë•Œ", "ì–´ë– í•œ", "ì–´ë–¤", "ì–´ë–¤ê²ƒ", "ì–´ë–¤ê²ƒë“¤", "ì–´ë–»ê²Œ", "ì–´ë–»í•´", "ì–´ì´", "ì–´ì§¸ì„œ", "ì–´ì¨‹ë“ ", "ì–´ì©Œë¼ê³ ", "ì–´ì©Œë©´", "ì–´ì©Œë©´ í•´ë„", "ì–´ì©Œë‹¤", "ì–´ì©”ìˆ˜ ì—†ë‹¤", "ì–´ì°Œ", "ì–´ì°Œëë“ ", "ì–´ì°Œëì–´", "ì–´ì°Œí•˜ë“ ì§€", "ì–´ì°Œí•˜ì—¬", "ì–¸ì œ", "ì–¸ì  ê°€", "ì–¼ë§ˆ", "ì–¼ë§ˆ ì•ˆ ë˜ëŠ” ê²ƒ", "ì–¼ë§ˆê°„", "ì–¼ë§ˆë‚˜", "ì–¼ë§ˆë“ ì§€", "ì–¼ë§ˆë§Œí¼", "ì–¼ë§ˆí¼", "ì—‰ì—‰", "ì—", "ì— ê°€ì„œ", "ì— ë‹¬ë ¤ ìˆë‹¤", "ì— ëŒ€í•´", "ì— ìˆë‹¤", "ì— í•œí•˜ë‹¤", "ì—ê²Œ", "ì—ì„œ", "ì—¬", "ì—¬ê¸°", "ì—¬ëŸ", "ì—¬ëŸ¬ë¶„", "ì—¬ë³´ì‹œì˜¤", "ì—¬ë¶€", "ì—¬ì„¯", "ì—¬ì „íˆ", "ì—¬ì°¨", "ì—°ê´€ë˜ë‹¤", "ì—°ì´ì„œ", "ì˜", "ì˜ì°¨", "ì˜†ì‚¬ëŒ", "ì˜ˆ", "ì˜ˆë¥¼ ë“¤ë©´", "ì˜ˆë¥¼ ë“¤ìë©´", "ì˜ˆì»¨ëŒ€", "ì˜ˆí•˜ë©´", "ì˜¤", "ì˜¤ë¡œì§€", "ì˜¤ë¥´ë‹¤", "ì˜¤ìë§ˆì", "ì˜¤ì§", "ì˜¤í˜¸", "ì˜¤íˆë ¤", "ì™€", "ì™€ ê°™ì€ ì‚¬ëŒë“¤", "ì™€ë¥´ë¥´", "ì™€ì•„", "ì™œ", "ì™œëƒí•˜ë©´", "ì™¸ì—ë„", "ìš”ë§Œí¼", "ìš”ë§Œí•œ ê²ƒ", "ìš”ë§Œí•œê±¸", "ìš”ì»¨ëŒ€", "ìš°ë¥´ë¥´", "ìš°ë¦¬", "ìš°ë¦¬ë“¤", "ìš°ì„ ", "ìš°ì— ì¢…í•©í•œê²ƒê³¼ê°™ì´", "ìš´ìš´", "ì›”", "ìœ„ì—ì„œ ì„œìˆ í•œë°”ì™€ê°™ì´", "ìœ„í•˜ì—¬", "ìœ„í•´ì„œ", "ìœ™ìœ™", "ìœ¡", "ìœ¼ë¡œ", "ìœ¼ë¡œ ì¸í•˜ì—¬", "ìœ¼ë¡œì„œ", "ìœ¼ë¡œì¨", "ì„", "ì‘", "ì‘ë‹¹", "ì˜", "ì˜ê±°í•˜ì—¬", "ì˜ì§€í•˜ì—¬", "ì˜í•´", "ì˜í•´ë˜ë‹¤", "ì˜í•´ì„œ", "ì´", "ì´ ë˜ë‹¤", "ì´ ë•Œë¬¸ì—", "ì´ ë°–ì—", "ì´ ì™¸ì—", "ì´ ì •ë„ì˜", "ì´ê²ƒ", "ì´ê³³", "ì´ë•Œ", "ì´ë¼ë©´", "ì´ë˜", "ì´ëŸ¬ì´ëŸ¬í•˜ë‹¤", "ì´ëŸ¬í•œ", "ì´ëŸ°", "ì´ëŸ´ì •ë„ë¡œ", "ì´ë ‡ê²Œ ë§ì€ ê²ƒ", "ì´ë ‡ê²Œë˜ë©´", "ì´ë ‡ê²Œë§í•˜ë©´", "ì´ë ‡êµ¬ë‚˜", "ì´ë¡œ ì¸í•˜ì—¬", "ì´ë¥´ê¸°ê¹Œì§€", "ì´ë¦¬í•˜ì—¬", "ì´ë§Œí¼", "ì´ë²ˆ", "ì´ë´", "ì´ìƒ", "ì´ì–´ì„œ", "ì´ì—ˆë‹¤", "ì´ì™€ ê°™ë‹¤", "ì´ì™€ ê°™ì€", "ì´ì™€ ë°˜ëŒ€ë¡œ", "ì´ì™€ê°™ë‹¤ë©´", "ì´ì™¸ì—ë„", "ì´ìš©í•˜ì—¬", "ì´ìœ ë§Œìœ¼ë¡œ", "ì´ì  ", "ì´ì§€ë§Œ", "ì´ìª½", "ì´ì²œêµ¬", "ì´ì²œìœ¡", "ì´ì²œì¹ ", "ì´ì²œíŒ”", "ì¸ ë“¯í•˜ë‹¤", "ì¸ì  ", "ì¼", "ì¼ê²ƒì´ë‹¤", "ì¼ê³±", "ì¼ë‹¨", "ì¼ë•Œ", "ì¼ë°˜ì ìœ¼ë¡œ", "ì¼ì§€ë¼ë„", "ì„ì— í‹€ë¦¼ì—†ë‹¤", "ì…ê°í•˜ì—¬", "ì…ì¥ì—ì„œ", "ì‡ë”°ë¼", "ìˆë‹¤", "ì", "ìê¸°", "ìê¸°ì§‘", "ìë§ˆì", "ìì‹ ", "ì ê¹", "ì ì‹œ", "ì €", "ì €ê²ƒ", "ì €ê²ƒë§Œí¼", "ì €ê¸°", "ì €ìª½", "ì €í¬", "ì „ë¶€", "ì „ì", "ì „í›„", "ì ì—ì„œ ë³´ì•„", "ì •ë„ì— ì´ë¥´ë‹¤", "ì œ", "ì œê°ê¸°", "ì œì™¸í•˜ê³ ", "ì¡°ê¸ˆ", "ì¡°ì°¨", "ì¡°ì°¨ë„", "ì¡¸ì¡¸", "ì¢€", "ì¢‹ì•„", "ì¢ì¢", "ì£¼ë£©ì£¼ë£©", "ì£¼ì €í•˜ì§€ ì•Šê³ ", "ì¤„ì€ ëª°ëë‹¤", "ì¤„ì€ëª¨ë¥¸ë‹¤", "ì¤‘ì—ì„œ", "ì¤‘ì˜í•˜ë‚˜", "ì¦ˆìŒí•˜ì—¬", "ì¦‰", "ì¦‰ì‹œ", "ì§€ë“ ì§€", "ì§€ë§Œ", "ì§€ë§ê³ ", "ì§„ì§œë¡œ", "ìª½ìœ¼ë¡œ", "ì°¨ë¼ë¦¬", "ì°¸", "ì°¸ë‚˜", "ì²«ë²ˆì§¸ë¡œ", "ì³‡", "ì´ì ìœ¼ë¡œ", "ì´ì ìœ¼ë¡œ ë§í•˜ë©´", "ì´ì ìœ¼ë¡œ ë³´ë©´", "ì¹ ", "ì½¸ì½¸", "ì¾…ì¾…", "ì¿µ", "íƒ€ë‹¤", "íƒ€ì¸", "íƒ•íƒ•", "í† í•˜ë‹¤", "í†µí•˜ì—¬", "íˆ­", "í‰¤", "í‹ˆíƒ€", "íŒ", "íŒ”", "í½", "í„ë ", "í•˜", "í•˜ê²Œë ê²ƒì´ë‹¤", "í•˜ê²Œí•˜ë‹¤", "í•˜ê² ëŠ”ê°€", "í•˜ê³  ìˆë‹¤", "í•˜ê³ ìˆì—ˆë‹¤", "í•˜ê³¤í•˜ì˜€ë‹¤", "í•˜êµ¬ë‚˜", "í•˜ê¸° ë•Œë¬¸ì—", "í•˜ê¸° ìœ„í•˜ì—¬", "í•˜ê¸°ëŠ”í•œë°", "í•˜ê¸°ë§Œ í•˜ë©´", "í•˜ê¸°ë³´ë‹¤ëŠ”", "í•˜ê¸°ì—", "í•˜ë‚˜", "í•˜ëŠë‹ˆ", "í•˜ëŠ” ê¹€ì—", "í•˜ëŠ” í¸ì´ ë‚«ë‹¤", "í•˜ëŠ”ê²ƒë„", "í•˜ëŠ”ê²ƒë§Œ ëª»í•˜ë‹¤", "í•˜ëŠ”ê²ƒì´ ë‚«ë‹¤", "í•˜ëŠ”ë°”", "í•˜ë”ë¼ë„", "í•˜ë„ë‹¤", "í•˜ë„ë¡ì‹œí‚¤ë‹¤", "í•˜ë„ë¡í•˜ë‹¤", "í•˜ë“ ì§€", "í•˜ë ¤ê³ í•˜ë‹¤", "í•˜ë§ˆí„°ë©´", "í•˜ë©´ í• ìˆ˜ë¡", "í•˜ë©´ëœë‹¤", "í•˜ë©´ì„œ", "í•˜ë¬¼ë©°", "í•˜ì—¬ê¸ˆ", "í•˜ì—¬ì•¼", "í•˜ìë§ˆì", "í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´", "í•˜ì§€ ì•Šë„ë¡", "í•˜ì§€ë§ˆ", "í•˜ì§€ë§ˆë¼", "í•˜ì§€ë§Œ", "í•˜í•˜", "í•œ ê¹Œë‹­ì—", "í•œ ì´ìœ ëŠ”", "í•œ í›„", "í•œë‹¤ë©´", "í•œë‹¤ë©´ ëª°ë¼ë„", "í•œë°", "í•œë§ˆë””", "í•œì ì´ìˆë‹¤", "í•œì¼ ìœ¼ë¡œëŠ”", "í•œí•­ëª©", "í•  ë”°ë¦„ì´ë‹¤", "í•  ìƒê°ì´ë‹¤", "í•  ì¤„ ì•ˆë‹¤", "í•  ì§€ê²½ì´ë‹¤", "í•  í˜ì´ ìˆë‹¤", "í• ë•Œ", "í• ë§Œí•˜ë‹¤", "í• ë§ì •", "í• ë¿", "í• ìˆ˜ìˆë‹¤", "í• ìˆ˜ìˆì–´", "í• ì¤„ì•Œë‹¤", "í• ì§€ë¼ë„", "í• ì§€ì–¸ì •", "í•¨ê»˜", "í•´ë„ëœë‹¤", "í•´ë„ì¢‹ë‹¤", "í•´ë´ìš”", "í•´ì„œëŠ” ì•ˆëœë‹¤", "í•´ì•¼í•œë‹¤", "í•´ìš”", "í–ˆì–´ìš”", "í–¥í•˜ë‹¤", "í–¥í•˜ì—¬", "í–¥í•´ì„œ", "í—ˆ", "í—ˆê±±", "í—ˆí—ˆ", "í—‰", "í—‰í—‰", "í—ë–¡í—ë–¡", "í˜•ì‹ìœ¼ë¡œ ì“°ì—¬", "í˜¹ì‹œ", "í˜¹ì€", "í˜¼ì", "í›¨ì”¬", "íœ˜ìµ", "íœ´", "íí", "í¥", "í˜ì…ì–´"]

            # ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜
            def remove_stopwords(text):
                tokens = text.split()  # í…ìŠ¤íŠ¸ë¥¼ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í† í°í™”
                tokens_filtered = [word for word in tokens if not word in stopwords]  # ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ í† í°ë§Œ ì„ íƒ
                return " ".join(tokens_filtered)  # í† í°ë“¤ì„ ë‹¤ì‹œ ê³µë°±ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜

            # ì „ì²˜ë¦¬ í•¨ìˆ˜ ìˆ˜ì •
            def preprocess_text(text):
                text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
                text = re.sub(r'\n', ' ', text)  # ê°œí–‰ë¬¸ì ì œê±°
                text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ
                text = re.sub(r'[0-9]+ê°¤', 'ê°œì›”', text)  # ìˆ«ì+ê°¤ -> ìˆ«ìê°œì›”
                text = re.sub(r'[^ê°€-í£.]', ' ', text)  # í•œê¸€ê³¼ ë§ˆì¹¨í‘œë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°
                text = re.sub(r'\.\s+', '.', text)  # ë§ˆì¹¨í‘œ ë‹¤ìŒì˜ ê³µë°± ì œê±°
                text = re.sub(r'\.{2,}', '.', text)  # ì—¬ëŸ¬ ê°œì˜ ë§ˆì¹¨í‘œë¥¼ í•˜ë‚˜ë¡œ
                text = re.sub(r'\s+', ' ', text)  # ë‹¤ì‹œ í•œ ë²ˆ ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ
                text = remove_stopwords(text)  # ë¶ˆìš©ì–´ ì œê±°
                return text

            # ê¸°ì¡´ì˜ ì½”ë“œì—ì„œ 'text' ì»¬ëŸ¼ì„ ì „ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„ì— ë¶ˆìš©ì–´ ì œê±° ê¸°ëŠ¥ì´ ì¶”ê°€ë¨
            df.loc[:, 'text'] = df['text'].apply(preprocess_text)

            # í˜•íƒœì†Œ ë¶„ì„ê¸° ì„¤ì •
            def get_tokenizer(tokenizer_name):
                if tokenizer_name == "komoran":
                    return Komoran()
                elif tokenizer_name == "okt":
                    return Okt()
                elif tokenizer_name == "mecab":
                    return Mecab()
                elif tokenizer_name == "hannanum":
                    return Hannanum()
                elif tokenizer_name == "kkma":
                    return Kkma()
                else:
                    return Okt()

            tokenizer = get_tokenizer("okt")
            
            def pos_tagging_and_filter(text):
                pos_tokens = tokenizer.pos(text)
                filtered_tokens = [token for token, pos in pos_tokens if pos != 'Josa']
                return filtered_tokens
            
            # ì „ì²˜ë¦¬ ë° í˜•íƒœì†Œ ë¶„ì„ ì‹¤í–‰
            df['text'] = df['text'].apply(preprocess_text)
            df['tok_text'] = df['text'].apply(pos_tagging_and_filter)

            # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
            now = datetime.now()
            formatted_time = now.strftime("%Y%m%d_%H%M%S")
            filename = f"í˜•íƒœì†Œë¶„ì„_{formatted_time}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')

            # ì™„ë£Œ ë©”ì‹œì§€ ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬
            st.success('ë°ì´í„° ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')
            st.download_button('ê²°ê³¼ ë‹¤ìš´ë¡œë“œ', data=df.to_csv(index=False), file_name=filename, mime='text/csv')

            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            df = df[['keyword', 'date', 'text', 'tok_text']]
            
            st.success("ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.write(df)

            
    st.markdown("---")  # ì„¸ë¡œ êµ¬ë¶„ì„ 
    
    
    # GPT-2ë¡œ ì€í–‰ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜
    def is_bank_related_gpt2(text):
        prompt = f"ì´ í…ìŠ¤íŠ¸ëŠ” ì€í–‰, ê¸ˆìœµ ë“±ì— ê´€ë ¨ëœ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ? ì˜ˆ, ì•„ë‹ˆìš”ë¡œ ëŒ€ë‹µí•˜ì‹œì˜¤. '{text}'"
        inputs = tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=950, padding=True)
        try:
            outputs = headmodel.generate(inputs, max_length=inputs.shape[1] + 15, num_return_sequences=1)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            return "ì˜ˆ" in response
        except IndexError as e:
            st.write(f"Error: {e}, Input Text: {text}")
            return False
    
    #  train_and_predict_model í•¨ìˆ˜ê°€ ì •ì˜
    # ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ì„ ìœ„í•œ í•¨ìˆ˜
    def train_and_predict_model(model, X_labeled, y_labeled, X_unlabeled):
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        model.fit(X_labeled, y_labeled, epochs=10, batch_size=32, validation_split=0.2)
        return (model.predict(X_unlabeled) > 0.5).astype(int)


    # GPT-2 ëª¨ë¸ì—ì„œ ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
    def get_embeddings(input_ids):
        input_tensor = torch.tensor([input_ids], dtype=torch.long)
        with torch.no_grad():
            outputs = model(input_tensor)
        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

    #####################
    #ìœ íš¨ ë¬¸ì¥ ì‹ë³„ AI
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fe060f141-f071-4186-a13a-8e4943ab6393%2FUntitled.png?table=block&id=9191834a-8351-48a4-a6a7-37bb893f385c&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=870&userId=&cache=v2', width=500)
    
    uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type="csv")

    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.write("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:", df.head())

        # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')
        model = GPT2Model.from_pretrained("skt/kogpt2-base-v2")
        headmodel = GPT2LMHeadModel.from_pretrained("skt/kogpt2-base-v2")
        
        # ë ˆì´ë¸”ë§ì„ ìœ„í•œ ìƒ˜í”Œ ì¶”ì¶œ ë° ì„ë² ë”© ì¶”ì¶œ
        labeling_df = pd.DataFrame(df['text'].sample(frac=0.03))
        new_data = [get_embeddings(tokenizer.encode(text, truncation=True, max_length=1024, padding=True)) for text in labeling_df['text']]
        embeddings = np.array(new_data)
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings)

        # ë ˆì´ë¸”ë§ ì§„í–‰
        labeled_data = [(text, 1 if is_bank_related_gpt2(text) else 0) for text in labeling_df['text']]
        df_labeled = pd.DataFrame(labeled_data, columns=['text', 'Label'])
        df_merged = pd.merge(df, df_labeled, on='text', how='left')

        # ì´í•˜ ì½”ë“œëŠ” ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•˜ë©°, eval í•¨ìˆ˜ ì‚¬ìš©ì´ í•„ìš”í•œ ë¶€ë¶„ì— ëŒ€í•´ì„œëŠ” ì£¼ì˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # ë°ì´í„° ì¤€ë¹„ ë° ëª¨ë¸ í•™ìŠµ, ì˜ˆì¸¡ ì½”ë“œ...
    


        # LSTMê³¼ RNN í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        texts = df_merged['tok_text'].apply(eval).astype(str)
        labels = df_merged['Label']
        tokenizer = Tokenizer()
        tokenizer.fit_on_texts(texts)
        sequences = tokenizer.texts_to_sequences(texts)
        max_seq_length = max(len(seq) for seq in sequences)
        padded_sequences = pad_sequences(sequences, maxlen=max_seq_length)
        labeled_indices = labels.notna()
        unlabeled_indices = ~labeled_indices
        X_labeled = padded_sequences[labeled_indices]
        y_labeled = labels[labeled_indices].astype(int)
        X_unlabeled = padded_sequences[unlabeled_indices]

        # LSTMì„ í†µí•œ í•™ìŠµ ë° ì˜ˆì¸¡
        model_LSTM = Sequential([
            Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_seq_length),
            LSTM(128),
            Dense(1, activation='sigmoid')
        ])
        LSTM_pseudo_labels = train_and_predict_model(model_LSTM, X_labeled, y_labeled, X_unlabeled)
        df_merged.loc[unlabeled_indices, 'LSTM_Label'] = LSTM_pseudo_labels.flatten()

        # RNNì„ í†µí•œ í•™ìŠµ ë° ì˜ˆì¸¡
        model_RNN = Sequential([
            Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_seq_length),
            SimpleRNN(128),
            Dense(1, activation='sigmoid')
        ])
        RNN_pseudo_labels = train_and_predict_model(model_RNN, X_labeled, y_labeled, X_unlabeled)
        df_merged.loc[unlabeled_indices, 'RNN_Label'] = RNN_pseudo_labels.flatten()

        # LSTMê³¼ RNNì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ë‹¤ë¥¼ ê²½ìš°, GRUë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ë¶„ë¥˜
        different_indices = df_merged[(df_merged['LSTM_Label'] != df_merged['RNN_Label']) & (unlabeled_indices)].index
        if different_indices.any():
            X_different = padded_sequences[different_indices]
            model_GRU = Sequential([
                Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_seq_length),
                GRU(128),
                Dense(1, activation='sigmoid')
            ])
            model_GRU.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
            model_GRU.fit(X_labeled, y_labeled, epochs=10, batch_size=32, validation_split=0.2)
            gru_pseudo_labels = (model_GRU.predict(X_different) > 0.5).astype(int)
            df_merged.loc[different_indices, 'GRU_Label'] = gru_pseudo_labels.flatten()
            df_merged['final_label'] = np.where(df_merged['LSTM_Label'] == df_merged['RNN_Label'], df_merged['LSTM_Label'], df_merged['GRU_Label'])
        else:
            df_merged['final_label'] = df_merged['LSTM_Label']
        
        df_merged = df_merged[(df_merged['final_label'] != 0) & (df_merged['final_label'].notna())]
        st.write("ë ˆì´ë¸”ë§ëœ ë°ì´í„°:", df_merged)
        st.download_button(label="CSVë¡œ ë‹¤ìš´ë¡œë“œ", data=df_merged.to_csv(index=False).encode('utf-8'), file_name='labeled_data.csv', mime='text/csv')

        
        
    
    

        
            
            
elif menu == 'ğŸ“Ší´ëŸ¬ìŠ¤í„°ë§':
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F28c7399d-ab78-40ef-a2c4-7b713329de8e%2FUntitled.png?table=block&id=b93be364-a1f9-426c-8121-50b0ebb989fe&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', use_column_width=True)
   
    # st.title('í´ëŸ¬ìŠ¤í„°ë§')
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F1456364b-eefa-4158-b9d1-106f8356e69e%2FUntitled.png?table=block&id=7e20f50a-85d5-47af-b5cd-3971e0a2db96&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=850&userId=&cache=v2', width=900)
    # íŒŒì¼ ì—…ë¡œë“œ ë¶€ë¶„
    st.title("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("CSV file", type=['csv'])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        desired_columns = ['keyword', 'date', 'text', 'tok_text']
        df = df[desired_columns]
        df = df.dropna(subset=['text']).reset_index(drop=True)

        # Doc2Vec ëª¨ë¸ í›ˆë ¨
        tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(df['tok_text'])]
        model = Doc2Vec(vector_size=50, min_count=2, epochs=40)
        model.build_vocab(tagged_data)
        model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)
        df['vector'] = [model.dv[str(i)].tolist() for i in range(len(tagged_data))]

        # PCA ìˆ˜í–‰
        vector_array = np.array(df['vector'].tolist())
        pca = PCA(n_components=2)
        df[['PC1', 'PC2']] = pca.fit_transform(vector_array)

        # KMeans í´ëŸ¬ìŠ¤í„°ë§
        X = df[['PC1', 'PC2']]
        sse = []
        silhouette_coefficients = []
        for k in range(3, 11):
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(X)
            sse.append(kmeans.inertia_)
            score = silhouette_score(X, kmeans.labels_)
            silhouette_coefficients.append(score)
        top_k_indices = sorted(range(len(silhouette_coefficients)), key=lambda i: silhouette_coefficients[i], reverse=True)[:3]
        optimal_k = min([index + 3 for index in top_k_indices])

        # ìµœì ì˜ kë¡œ K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        kmeans = KMeans(n_clusters=optimal_k, random_state=42)
        df['Cluster'] = kmeans.fit_predict(X)

        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
        fig, ax = plt.subplots()
        scatter = ax.scatter(df['PC1'], df['PC2'], c=df['Cluster'], cmap='viridis', marker='o', edgecolor='black')
        legend1 = ax.legend(*scatter.legend_elements(), title="Clusters")
        ax.add_artist(legend1)
        st.pyplot(fig)

        # í´ëŸ¬ìŠ¤í„°ë³„ TF-IDF ì¶”ì¶œ ë° ìƒìœ„ ë‹¨ì–´
        okt = Okt()
        def tokenize(text):
            return okt.nouns(text)

        vectorizer = TfidfVectorizer(tokenizer=tokenize, max_features=1000)
        X_tfidf = vectorizer.fit_transform(df['text'])
        tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())
        top_tfidf_words_per_cluster = {}
        for cluster in range(optimal_k):
            cluster_indices = df[df['Cluster'] == cluster].index
            mean_tfidf = tfidf_df.loc[cluster_indices].mean(axis=0)
            top_words = mean_tfidf.nlargest(20)
            filtered_words = top_words[top_words.index.str.len() > 1]
            top_tfidf_words_per_cluster[cluster] = filtered_words
            
        # í´ëŸ¬ìŠ¤í„°ë³„ ìƒìœ„ TF-IDF ë‹¨ì–´ ì¶”ì¶œ
        okt = Okt()
        def tokenize(text):
            return okt.nouns(text)

        vectorizer = TfidfVectorizer(tokenizer=tokenize, max_features=1000)
        X_tfidf = vectorizer.fit_transform(df['text'])
        tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())
        
        top_tfidf_words_per_cluster = {}
        for cluster in range(optimal_k):
            cluster_indices = df[df['Cluster'] == cluster].index
            mean_tfidf = tfidf_df.loc[cluster_indices].mean(axis=0)
            top_words = mean_tfidf.nlargest(20)
            filtered_words = top_words[top_words.index.str.len() > 1]
            top_tfidf_words_per_cluster[cluster] = filtered_words

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = AutoModelForCausalLM.from_pretrained("MoaData/Myrrh_solar_10.7b_3.0").to(device)
        tokenizer = AutoTokenizer.from_pretrained("MoaData/Myrrh_solar_10.7b_3.0")
        
        # ì£¼ì œ ì…ë ¥ ë°›ê¸°
        topic = st.text_input("ë¶„ì„í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì€í–‰):")
        if topic:
            # ê´€ë ¨ì„± ê²€ì‚¬ í•¨ìˆ˜ ì •ì˜
            def check_relevance(words, topic):
                relevant_words = []
                model.eval()
                for word in words:
                    input_text = f"ì£¼ì œ: {topic}, ë‹¨ì–´: {word} - ì„œë¡œ ê´€ë ¨ ìˆë‚˜? ì˜ˆ ë˜ëŠ” ì•„ë‹ˆì˜¤ë¡œë§Œ ì§§ê²Œ ëŒ€ë‹µí•´ë¼."
                    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
                    outputs = model.generate(input_ids, max_length=60)
                    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    if "ì˜ˆ" in result:
                        relevant_words.append(word)
                return relevant_words

            # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë‹¨ì–´ ê²€ì‚¬ ë° ì¶œë ¥
            for cluster, words in top_tfidf_words_per_cluster.items():
                relevant_words = check_relevance(list(words.index), topic)
                st.write(f"í´ëŸ¬ìŠ¤í„° {cluster}ì—ì„œ ì£¼ì œ '{topic}'ê³¼ ê´€ë ¨ëœ ë‹¨ì–´ë“¤:", relevant_words)

            # ë¬¸ì¥ relevancy í•¨ìˆ˜
            model_sentence = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

            def find_most_relevant_sentence(texts, word, topic):
                relevant_texts = [text for text in texts if word in text]
                if not relevant_texts:
                    return "í•´ë‹¹ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ ì—†ìŒ"

                topic_embedding = model_sentence.encode([topic], convert_to_tensor=True)
                text_embeddings = model_sentence.encode(relevant_texts, convert_to_tensor=True)
                cos_scores = util.pytorch_cos_sim(topic_embedding, text_embeddings)[0]
                highest_score_index = torch.argmax(cos_scores).item()
                return relevant_texts[highest_score_index]

            # ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë¬¸ì¥ ì°¾ê¸°
            cluster_dataframes = {}
            for cluster in range(optimal_k):
                data = []
                cluster_texts = df[df['Cluster'] == cluster]['text'].tolist()
                words = list(top_tfidf_words_per_cluster[cluster].index)
                for word in words:
                    most_relevant_sentence = find_most_relevant_sentence(cluster_texts, word, topic)
                    data.append({
                        'word': word,
                        'sentence': most_relevant_sentence
                    })
                cluster_dataframes[cluster] = pd.DataFrame(data)
                st.write(f"í´ëŸ¬ìŠ¤í„° {cluster}ì˜ ë°ì´í„°:")
                st.dataframe(cluster_dataframes[cluster])

            # í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„±
            def generate_cluster_name(texts):
                combined_text = " ".join(texts)
                prompt = f"ë‹¤ìŒ ì„¤ëª…ì„ ë³´ê³  í•´ë‹¹ ìì—°ì–´ êµ°ì§‘ì— ì í•©í•œ ì´ë¦„ì„ í•˜ë‚˜ë§Œ ì§€ì–´ì£¼ì„¸ìš”: '{combined_text}'"
                inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)
                outputs = model.generate(inputs, max_length=50)
                name = tokenizer.decode(outputs[0], skip_special_tokens=True)
                return name

            cluster_names = []
            for cluster, frame in cluster_dataframes.items():
                texts = frame['sentence'].tolist()
                cluster_name = generate_cluster_name(texts)
                cluster_names.append({'Cluster': cluster, 'Name': cluster_name})

            clustername_df = pd.DataFrame(cluster_names)
            st.write("í´ëŸ¬ìŠ¤í„° ì´ë¦„:")
            st.dataframe(clustername_df)
    
    
    
       
    
elif menu == 'ğŸ“ˆì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„':
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fea4d3e84-e5e4-400b-b04d-5c85677f600f%2FUntitled.png?table=block&id=5ab154fa-6728-491f-bda0-ee026ed9ad51&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fe384251a-7f6a-4d60-ae2b-5a19493577ca%2F43105e6b-98c7-482a-aa41-2dbd834f1e72.png?table=block&id=24bbb6fa-6c9e-48ad-a0dd-590f78b7c047&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=500&userId=&cache=v2', width=900)
    
    
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False

    # Streamlit ì•± ì œëª©
    st.title("Text Data Clustering and Visualization")

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)

        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
        clusters = df.groupby('Cluster')

        # Okt ê°ì²´ ìƒì„±
        okt = Okt()

        # ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
        def extract_nouns(text):
            tokens = okt.nouns(text)
            return [token for token in tokens if len(token) > 1]

        # ë„¤íŠ¸ì›Œí¬ ìƒì„± í•¨ìˆ˜
        def create_network(texts):
            vectorizer = TfidfVectorizer(tokenizer=extract_nouns)
            tfidf_matrix = vectorizer.fit_transform(texts)
            cosine_sim = cosine_similarity(tfidf_matrix)
            words = vectorizer.get_feature_names_out()

            G = nx.Graph()
            n = min(len(words), cosine_sim.shape[0])
            for i in range(n):
                for j in range(i + 1, n):
                    if cosine_sim[i, j] > 0.1:
                        G.add_edge(words[i], words[j], weight=cosine_sim[i, j])
            return G

        # ì»¤ë®¤ë‹ˆí‹° íƒì§€ ë° ì•„ì´ê²ë²¡í„° ê³„ì‚°
        def detect_communities_and_eigenvector(G, resolution=0.9):
            import networkx.algorithms.community as nx_comm
            # resolution íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•˜ì—¬ ì»¤ë®¤ë‹ˆí‹°ì˜ ìˆ˜ë¥¼ ì¡°ì •
            communities = nx_comm.louvain_communities(G, resolution=resolution)
            eigenvector = nx.eigenvector_centrality_numpy(G)
            return communities, eigenvector

        # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
        def visualize_network(G, eigenvector, communities, cluster_id):
            pos = nx.spring_layout(G, seed=42)
            colors = list(mcolors.TABLEAU_COLORS.keys())
            community_color = {node: colors[i % len(colors)] for i, comm in enumerate(communities) for node in comm}
            base_size = 10000
            scale_factor = 2
            node_size = [eigenvector[node] * base_size * (scale_factor if eigenvector[node] > np.median(list(eigenvector.values())) else 1) for node in G]
            labels = {node: node if eigenvector[node] > np.median(list(eigenvector.values())) else '' for node in G}
            plt.figure(figsize=(20, 20))
            nx.draw_networkx(G, pos, node_color=[community_color[node] for node in G],
                            node_size=node_size, labels=labels, font_size=7, with_labels=True, edge_color='gray', font_family='Malgun Gothic')
            plt.title(f'Network Graph for Cluster {cluster_id}')
            plt.axis('off')
            st.pyplot(plt)

        # í´ëŸ¬ìŠ¤í„°ë³„ ì²˜ë¦¬ ë° ì‹œê°í™” ì‹¤í–‰
        for cluster_id, group in clusters:
            st.subheader(f"Cluster {cluster_id}")
            texts = group['text'].values
            G = create_network(texts)
            communities, eigenvector = detect_communities_and_eigenvector(G)
            visualize_network(G, eigenvector, communities, cluster_id)

        # í´ëŸ¬ìŠ¤í„°ë³„ ì²˜ë¦¬ ë° ì£¼ìš” í‚¤ì›Œë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
        for cluster_id, group in clusters:
            st.subheader(f"Cluster {cluster_id}")
            texts = group['text'].values
            G = create_network(texts)
            communities, eigenvector = detect_communities_and_eigenvector(G)
            # ê° ì»¤ë®¤ë‹ˆí‹°ë³„ ë°ì´í„° í”„ë ˆì„ ìƒì„± ë° ì €ì¥
            for i, community in enumerate(communities):
                community_words = {word: eigenvector[word] for word in community}
                community_df = pd.DataFrame(list(community_words.items()), columns=['Word', 'Eigenvector'])
                # íŒŒì¼ëª… ì§€ì • (í´ëŸ¬ìŠ¤í„°ì™€ ì»¤ë®¤ë‹ˆí‹° ë²ˆí˜¸ í¬í•¨)
                filename = f'cluster_{cluster_id}_Actor_{i}_keywords.csv'
                # ë°ì´í„° í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ì €ì¥
                community_df.to_csv(filename, index=False)
                st.write(f'Data for Cluster {cluster_id}, Community {i} saved to {filename}.')
                # ë°ì´í„° í”„ë ˆì„ í‘œì‹œ
                st.dataframe(community_df)
                
        # ë„¤íŠ¸ì›Œí¬ ìƒì„± ë° ì»¤ë®¤ë‹ˆí‹° íƒì§€ í•¨ìˆ˜
        def create_network_and_detect_communities(texts):
            vectorizer = TfidfVectorizer(tokenizer=extract_nouns)
            tfidf_matrix = vectorizer.fit_transform(texts)
            cosine_sim = cosine_similarity(tfidf_matrix)
            words = vectorizer.get_feature_names_out()
            G = nx.Graph()
            n = min(len(words), cosine_sim.shape[0])
            for i in range(n):
                for j in range(i + 1, n):
                    if cosine_sim[i, j] > 0.1:
                        G.add_edge(words[i], words[j], weight=cosine_sim[i, j])
            communities = nx.algorithms.community.louvain_communities(G, resolution=1)
            return communities, words, G

        # í´ëŸ¬ìŠ¤í„°ë³„ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
        cluster_community_dataframes = defaultdict(list)
        for cluster_id, group in clusters:
            st.subheader(f"Processing Cluster {cluster_id}")
            texts = group['text'].values
            communities, words, G = create_network_and_detect_communities(texts)
            for idx, community in enumerate(communities):
                pattern = '|'.join([f"\\b{word}\\b" for word in community if word in words])
                community_df = group[group['text'].str.contains(pattern, regex=True)]
                community_df = community_df.reset_index(drop=True)
                filename = f"cluster_{cluster_id}_Actor_{idx}.csv"
                community_df.to_csv(filename, index=False)
                cluster_community_dataframes[cluster_id].append(community_df)
                st.write(f'Actor {idx} Data for Cluster {cluster_id} saved to {filename}.')
                st.dataframe(community_df)

elif menu == 'ğŸ“‰í† í”½ ëª¨ë¸ë§':
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2Fcefff13e-14df-4394-8bca-6a8ab9f58534%2FUntitled.png?table=block&id=06bd4623-7c03-4811-b2c1-d6c3d1657b05&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', use_column_width=True)
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F114f8151-802b-4f9b-a95e-98072a74a92a%2FUntitled.png?table=block&id=c8cf6409-bde9-458a-999a-6c09213d3fc0&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1290&userId=&cache=v2', width=900)
   
 
    # Set Korean font
    @st.cache_data
    def set_korean_font():
        nanum_gothic_font_path = 'C:/WINDOWS/Fonts/NGULIM.TTF'
        font_name = fm.FontProperties(fname=nanum_gothic_font_path).get_name()
        plt.rc('font', family=font_name)

    set_korean_font()

    # Function to extract nouns
    def extract_nouns(text):
        okt = Okt()
        return okt.nouns(text)

    # Function to perform LDA
    def lda_modeling(noun_texts, random_seed=42):
        dictionary = corpora.Dictionary(noun_texts)
        if len(dictionary) == 0:
            return None, None, None
        corpus = [dictionary.doc2bow(text) for text in noun_texts]
        coherence_values = []
        for num_topics in range(2, 8):
            model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=random_seed)
            coherence_model = CoherenceModel(model=model, texts=noun_texts, dictionary=dictionary, coherence='c_v')
            coherence_values.append(coherence_model.get_coherence())
        if coherence_values:
            optimal_num_topics = np.argmax(coherence_values) + 2
            model = models.LdaModel(corpus, num_topics=optimal_num_topics, id2word=dictionary, passes=10, random_state=random_seed)
            return model, corpus, dictionary
        return None, None, None

    def main():
        st.title("Topic Modeling and Visualization")

        uploaded_file = st.file_uploader("Choose a file", type=['csv'])
        
        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            df['nouns'] = df['text'].apply(extract_nouns)
            model, corpus, dictionary = lda_modeling(df['nouns'])
            if model and corpus and dictionary:
                lda_display = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)
                lda_html = pyLDAvis.prepared_data_to_html(lda_display)
                components.html(lda_html, height=800)  # Use Streamlit components to render HTML
                
                # Save topic data and keywords
                for topic_id in range(model.num_topics):
                    words = model.show_topic(topic_id, topn=10)
                    topic_df = pd.DataFrame(words, columns=['keyword', 'weight'])
                    st.download_button(label=f"Download keywords for Topic {topic_id}", data=topic_df.to_csv(index=False), file_name=f"topic_{topic_id}_keywords.csv", mime='text/csv')
            else:
                st.error("Insufficient data for LDA")

    if __name__ == "__main__":
        main()

     
            
elif menu == 'ğŸ’»ê¸°íšŒì ìˆ˜':
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F480fb0aa-9cc7-4554-b6ec-28d5821c5bbc%2FUntitled.png?table=block&id=fb560845-d5e5-45c7-b140-9f8b5b007ca1&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=1390&userId=&cache=v2', use_column_width=True)
   
    # st.title('ê¸°íšŒì ìˆ˜')
    
# ê²Œì‹œíŒ ë¶€ë¶„(ì»¤ë®¤ë‹ˆí‹°)
elif menu == "ğŸ™ğŸ»DGB CAS ì»¤ë®¤ë‹ˆí‹°":
    st.image('https://brassy-dolphin-f22.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fa60d437f-ec9f-4f91-a164-6f05ff0d8852%2F45af589f-3fc4-475a-82bb-e1c1a4d6a067%2FUntitled.png?table=block&id=99ceb2ea-bf6a-484c-b3b8-381c352b6e49&spaceId=a60d437f-ec9f-4f91-a164-6f05ff0d8852&width=870&userId=&cache=v2',use_column_width=True)
    
    #
    # st.title('ê²Œì‹œíŒ')
    post_title = st.text_input('ì œëª©')
    post_content = st.text_area('ë‚´ìš©', height=250)
    uploaded_file = st.file_uploader("íŒŒì¼ì„ ì²¨ë¶€í•˜ì„¸ìš”.", type=['jpg', 'png', 'pdf', 'csv', 'xlsx'])
    submit_button = st.button('ê²Œì‹œí•˜ê¸°')

    if submit_button:
        # íŒŒì¼ëª… ì²˜ë¦¬
        file_name = uploaded_file.name if uploaded_file is not None else "íŒŒì¼ ì—†ìŒ"
        # ì„¸ì…˜ ìƒíƒœì— ê²Œì‹œê¸€ ì •ë³´ ì¶”ê°€
        st.session_state['posts'].append({'ì œëª©': post_title, 'ë‚´ìš©': post_content, 'íŒŒì¼ëª…': file_name})
        st.success('ê²Œì‹œê¸€ì´ ì„±ê³µì ìœ¼ë¡œ ì˜¬ë¼ê°”ìŠµë‹ˆë‹¤!')

    # ê²Œì‹œê¸€ ì¶œë ¥
    for i, post in enumerate(st.session_state['posts']):
        st.write(f"ì œëª©: {post['ì œëª©']}")
        st.write(f"ë‚´ìš©: {post['ë‚´ìš©']}")
        st.write(f"ì²¨ë¶€íŒŒì¼: {post['íŒŒì¼ëª…']}")
        if st.button('ì‚­ì œ', key=f'delete_{i}'):
            # í•´ë‹¹ ê²Œì‹œê¸€ ì‚­ì œ
            del st.session_state['posts'][i]
            st.experimental_rerun()  # í™”ë©´ ìƒˆë¡œê³ ì¹¨

   